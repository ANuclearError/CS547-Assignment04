\documentclass[11pt, a4paper]{article}
\usepackage[parfill]{parskip}
\usepackage[margin=0.75in]{geometry}
\usepackage{url}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{float}
\usepackage{tabularx}
\usepackage{subfiles}

\lstdefinestyle{snippet}{
    numbers = left,
    basicstyle = \ttfamily\footnotesize,
    frame = single,
    breaklines = true,
}

\setlength\parindent{0pt}

\begin{document}
\title{CS547 Advanced Topics in Computer Science\\
\large{Assignment 04 - Genetic Programming: Software Cost Estimation}}
\author{Aidan O'Grady - 201218150}
\date{}
\maketitle
\section{Introduction} % (fold)
\label{sec:introduction}
The purpose of this assignment was to utilise genetic programming in approaching
the problem of software cost estimation. When a software project is being
proposed, there are multiple factors that can influence the eventual cost of the
software. It can be difficult to determine which factors are more important to
consider, making this an appropriate search challenge in the scope of genetic
programming. The goal of this assignment was to use genetic programming to
evolve programs to estimate costs of given datasets based on the attributes
with, comparing the resulting program's predictions with methods such as linear
regression.
% section introduction (end)

\section{Background} % (fold)
\label{sec:background}
There were around 15 .arff files available for use as part of this assignment.
Of those, I settled on using the \emph{Albrecht, Kemerer} \& \emph{Miyazaki}
datasets as part of my work. These three were chosen due to their relative
straightforwardness compared to other files. These datasets were also used by
Dolado \cite{Dolado200161} in a related study, which included results obtained
from his own genetic programming experimentation (providing me with references
to help understand the datasets). Knowing that I could use his results as a
reference point helped in ensuring that I stayed on the right track with this
problem. For determining which attributes to focus on for the datasets, I
decided to place my focus on numeric attributes, since I felt that including
the string or date attributes would have been difficult to include in the
calculations created by the genetic program.

\subsection{Albrecht} % (fold)
\label{sub:albrecht}
With the Albrecht dataset, I was able to understand the `function point' concept
used in various datasets. The original paper by Albrecht \& Gaffney
\cite{1703110}, showed how the \emph{RawFPcounts} attribute was calculated
from the \emph{Input, Output, Inquiry \& File} attributes, with a multiplier
then provided as well to add a modified value as well. With this dataset, I
decided to ignore the adjusted figure, instead opting to include the raw
function point value and its multiplier. I also opted to include the attributes
that make up the function points value, in case that the function points alone
could not provide the most accurate Prediction.

Included: \emph{Input, Output, Inquiry, File, FPAdj, RawFPcounts}

Excluded: \emph{AdjFP}

Effort: \emph{Effort}
% subsection albrecht (end)

\subsection{Kemerer} % (fold)
\label{sub:kemerer}
The Kemerer dataset also included both raw and adjusted function point counts,
so the adjusted value was again ignored for the sake of consistency. The
\emph{ID} was ignored due to only being an identifier.

This dataset also included two attributes, \emph{Language} \& \emph{Hardware},
that acted as enumerations. This had to be addressed, since while these would be
factors in the real world, I was not certain whether I could trust the
enumerated values to be reliable in providing cost estimations. Thus, these were
also excluded. \emph{Duration} \emph{KSLOC} were excluded after noticing that
the Dolado paper did not include them. Running tests with these attributes found
that \emph{KSLOC} would achieve meaningful improvements while \emph{Duration}'s
inclusion did not cause significant change.

Included: \emph{RAWFP}

Excluded: \emph{ID, Language, Hardware, Duration, KSLOC, AdjFP}

Effort: \emph{EffortMM}
% subsection kemerer (end)

\subsection{Miyazaki} % (fold)
\label{sub:miyazaki}
As with \emph{Kemerer}, the \emph{ID} and \emph{KLOC} attributes were excluded
for the same reasons. This meant that the only real decision to be made with
this set is which group of three attributes to choose. From reading the original
source \cite[Section~4.2]{MIYAZAKI19943}, I decided it was best to try with both
and choose whichever performed better with genetic programming. This became
\emph{SCRN, FORM} and \emph{FILE}. It may have been worth investigating some
combinations between them as well however.

Included: \emph{SCRN, FORM, FILE}

Excluded: \emph{ID, KLOC, ESCRN, EFORM, EFILE}

Effort: \emph{MM}
% subsection miyazaki (end)
% section background (end)

\section{Implementation Details} % (fold)
\label{sec:implementation_details}
The implementation of solving this problem came under three main section;
parsing the datasets, storing the values of these datasets into classes, and the
genetic programming parts as well. The genetic programming was handled with the
JGAP framework, meaning I was only required to implement the fitness function
and problem scope for it as well. The linear regression was performed using
Weka.

\subsection{Storing Data} % (fold)
\label{sub:storing_data}
The .arff files being used consist of three main aspects: the name of the
relation, the attributes and the data itself. As such, it was best to create two
classes for storing this data, \emph{DataRecord} and \emph{DataSet}.

\subsubsection{DataRecord} % (fold)
\label{ssub:datarecord}
This class was responsible for storing the data from a single record of the data
in the file. The class stored the actual effort of the record in addition to the
additional attributes. I felt that a \emph{Map} was the best way of storing a
record, using the name of the attribute as the key. I believed this allowed for
easier access to the values from just using the key, which I could use as
variable names in JGAP. As a result, the class also contains a method for
putting key/value pairs into the map.
% subsubsection datarecord (end)

\subsubsection{DataSet} % (fold)
\label{ssub:dataset}
The \emph{DataSet} class contained all the data within the file. As well as
including the name of the dataset for the sake of completion, two \emph{List}s
were stored for the attributes and data. As a result, methods for this class
were used for adding to getting from this list.
% subsubsection dataset (end)
% subsection storing_data (end)

\subsection{Parsing Data} % (fold)
\label{sub:parsing_data}
The \emph{DataParser} class requires three pieces of information in order to
parse the file: the file location, the list of attributes the user wishes to
include in the genetic program, and the attribute that the program is trying to
calculate. The list of attributes required for \emph{Data} is easily obtained,
since each relevant line starts with `@attribute', so the only concern was
ensuring that the attribute was numeric, achieved for string splitting.

Since the user is only concerned with a subset of the attributes, it was
important to find their indices in the attribute list, so that when the `@data'
section of the file was parsed, the correct values were being added to the
\emph{DataRecord} objects. Should the parser fail to parse, the parser simply
returns null, to be handled elsewhere.
% subsection parsing_data (end)

\subsection{JGAP} % (fold)
\label{sub:jgap}
The JGap implementation was based on a tutorial \cite{JGapTutorial} we were
directed to, with modifications to fit this problem scope. The JGap section was
made up of two parts, the \emph{CostProblem} and \emph{CostFitnessFunction}.

\subsubsection{CostProblem} % (fold)
\label{ssub:costproblem}
This class outlined the basic problem, containing the configuration of elements
such as the population size and program depths. The \emph{DataSet}'s attributes
were converted into a list of \emph{Variables} and the function set used in the
generated programs was expanded to addition, subtraction, multiplication,
division, power, exponential and logarithmic functions. Finally, the terminal
range was changed to floating point numbers between 0 and 100 for use as
functions.
% subsubsection costproblem (end)

\subsubsection{CostFitnessFunction} % (fold)
\label{ssub:costfitnessfunction}
The fitness function class is extremely simple. The fitness of a program
generated by JGAP is the \emph{Mean Magnitude of Relative Error} (MMRE) metric,
defined as follows:

\[\frac{\sum_{i = 1}^{n} \frac{abs(actual_i - estimated_i)}{actual_i}}{n}\]

Where \(n\) is the number of records in the dataset, \(actual_i\) is the effort
contained in the file for record \(i\) and \(estimated_i\) is the effort
calculated by the program. The fitness function ensures that the variables
are correctly set before each execution as well. The lower the fitness value,
the better it is.
% subsubsection costfitnessfunction (end)
% subsection jgap (end)

\subsection{Application} % (fold)
\label{sub:application}
The \emph{Application} class commenced the evolution process. After parsing and
the \emph{CostProblem} is defined, JGAP performs evolution for 100 generations,
with time taken being recorded. When the evolution has finished, the best
performing program created is taken, and is used on the dataset once again to
obtain the Predicted costs for each \emph{DataRecord}. This allows for the
calculation of \(Pred(n)\) scores where \(n = \{10, 25, 50\}\), allowing for a
final output outlining how many of the estimations fall within 10\%, 25\% or
50\% of the actual cost.

100 generations was chosen since it was found that anything beyond that saw
marginal improvements. For example, if the number of generations was set to 500,
then the number of improvements from generations 101 to 500 was too small to
justify the much larger length of time the evolution took.
% subsection application (end)
% section implementation_details (end)
\newpage
\section{Results} % (fold)
\label{sec:results}
These results are summation of the overall performance of JGap and Weka on the
three datasets. Detailed results can be found in Appendix
\ref{sec:detailed_results}, showing each data record's performance.
Constants have been rounded to two decimal places, with minimal simplification
of the returned programs applied.

The JGap configuration saw a population size of 5000, a maximum initial depth of
10, and a maximum crossover depth of 20. The population was evolved for 100
generations.

\subsection{Albrecht Results} % (fold)
\label{sub:albrecht_results}
Number of records: 24

GP:
\(((RawFPcounts / 66.2) + ((log (log (Output * log (File)))) +
(FPAdj ^ {Inquiry}))) * log (log (File))\)

LR:
\(0.16 * Input + 0.33 * Output + 0.50 * Inquiry + 0.35 * File - 14.87\)

\begin{tabularx}{\textwidth}{| l | X | X |}
\hline
 & \textbf{Genetic Program} & \textbf{Linear Regression} \\
\hline
\emph{MMRE} & 0.351 & 0.883 \\
\hline
\emph{Pred(10)} & 08/24 (33.3\%) & 05/24 (20.8\%) \\
\emph{Pred(25)} & 13/24 (54.2\%) & 09/24 (37.5\%) \\
\emph{Pred(50)} & 18/24 (75.0\%) & 12/24 (50.0\%) \\
\hline
\emph{Time (s)} & 19.426 & 0.01 \\
\hline
\end{tabularx}
% subsection albrecht_results (end)

\subsection{Kemerer Results} % (fold)
\label{sub:kemerer_results}
Number of records: 15

GP: \(log(RAWFP^{(Exp((RAWFP / 54.47)))}) + (log (RAWFP) + log (RAWFP) + RAWFP +
RAWFP) / 41.06)\)

LR: \(0.33 \times RAWFP - 111.37\)

\begin{tabularx}{\textwidth}{| l | X | X |}
\hline
 & \textbf{Genetic Program} & \textbf{Linear Regression} \\
\hline
\emph{MMRE} & 0.342 & 0.874 \\
\hline
\emph{Pred(10)} & 04/15 (26.7\%) & 00/15 (00.0\%) \\
\emph{Pred(25)} & 07/15 (46.7\%) & 03/15 (20.0\%) \\
\emph{Pred(50)} & 10/15 (66.7\%) & 06/15 (40.0\%) \\
\hline
\emph{Time (s)} & 19.426 & 0.01 \\
\hline
\end{tabularx}
% subsection kemerer_results (end)

\subsection{Miyazaki Results} % (fold)
\label{sub:miyazaki_results}
Number of records: 48

GP: \((Exp(0.38)) + ((((Exp((Exp(0.5)))) + FORM) / (1.27 + SCRN)) + SCRN)\)

LR: \(0.4445 * SCRN + 0.84 * FORM + 3.59 * FILE - 71.13\)

\begin{tabularx}{\textwidth}{| l | X | X |}
\hline
 & \textbf{Genetic Program} & \textbf{Linear Regression} \\
\hline
\emph{MMRE} & 0.410 & 1.942 \\
\hline
\emph{Pred(10)} & 11/48 (22.9\%) & 02/48 (04.2\%) \\
\emph{Pred(25)} & 19/48 (39.6\%) & 08/48 (16.7\%) \\
\emph{Pred(50)} & 27/48 (56.3\%) & 13/48 (27.1\%) \\
\hline
\emph{Time (s)} & 22.039 & 0.02 \\
\hline
\end{tabularx}
% subsection miyazaki_results (end)
% section results (end)

\section{Comparisons} % (fold)
\label{sec:comparisons}
\subsection{Albrecht} % (fold)
\label{sub:albrecht_comp}
\emph{Albrecht} saw strong results with a third of estimations being within 10\%
and a majority being within 25\%, with an MMRE of 0.351 being achieved through
GP. LR was far weaker, with an MMRE of 0.883 and only 20\% of estimations being
within 10\% of the actual result (37.5\% within 25\%). Of the 24 records, 1 saw
MRE (Magnitude of relative error) greater than 1 with GP, while LR had 8 of the
24 being greater than one (including one with an MRE of 7.8). This combined with
a few instances of negative predictions shows the poor performance of linear
regression.

The program produced through genetic algorithms requires 11 operations, while
linear regression's required 8. GP did not use the \emph{Input} attribute while
LR did not use \emph{RawFPCounts} or \emph{FPAdj}, showing that these results
were influenced by these different focuses on attributes. In addition, GP used
more complex operations such as \emph{log} and powers.
% subsection albrecht_comp (end)

\subsection{Kemerer} % (fold)
\label{sub:kemerer_comp}
\emph{Kemerer} saw more strong performances, with GP predicting a quarter of
records within 10\% of the actual cost, while LR failed to predict any within
this range. GP also performed twice as well predicting within 25\% of the actual
cost, again showing superiority. The MMRE scores again show GP having half the
average of LR, being yet another victory. None of GP's predictions had MRE
scores greater than 1, while 4 of LR's 15 predictions did.

Again, the program produced by GP is more complex, with 10 operations to LR's 2,
with GP again using more complex operations such as \emph{log}, \emph{exp} and
powers. The sheer complexity of GP's result is astounding, although once again,
it is this complexity that creates superior results.
% subsection kemerer_comp (end)

\subsection{Miyazaki} % (fold)
\label{sub:miyazaki_comp}
Once again, JGap vastly outperformed Weka with the \emph{Miyazaki} dataset.
GP's MMRE score was almost 5 times better than the MMRE score of LR's, with only
a single record of the 48 having an MRE score greater than 1 while the majority
of predictions derived from linear regression did.

Almost a quarter of GP's predictions fell within a 10\% range while less than
5\% did for LR's results. Similarly, almost 40\% fell within 25\% for GP and
16.7\% did for LR, another strong performed.

When looking at the program itself, the first notable feature is JGap's ignoring
of the \emph{FILE} attribute. What is also interesting that JGap's solution only
used a single extra operation compared to Weka, although it should be noted that
JGap did include more complex operations, also requiring three more operations
than LR's program.
% subsection miyazaki_comp (end)

\subsection{Conclusions} % (fold)
\label{sub:conclusions}
The overall conclusion is that JGap's genetic programming overwhelmingly
outperformed Weka's linear regression. Genetic programming can typically
estimate somewhere 20\% and 33\% of all records within 10\%, and between 40\%
and 60\% within 25\%. While JGap was prone to the very rare abnormally
inaccurate estimation this was a very frequent issue for Weka to deal with with
its produced results. 

JGap required more processing time to come up with its best results, taking
advantage of more complex functions in order to achieve its better results with
more complex programs being produced. However, these complex functions can just
be placed inside some script, so it is not completely essential that they be
easily readable for humans, merely appreciated. 
% subsection conclusions (end)
% section comparisons (end)

\newpage
\appendix
\section{Detailed Results} % (fold)
\label{sec:detailed_results}
Note that the ID field refers to the order of records in the file, not the ID
attribute within the file.

\subfile{res/albrecht}
\subfile{res/kemerer}
\subfile{res/miyazaki}
% section detailed_results (end)
\bibliographystyle{plain}
\bibliography{report} 
\end{document}