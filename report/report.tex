\documentclass[11pt, a4paper]{article}
\usepackage[parfill]{parskip}
\usepackage[margin=0.66in]{geometry}
\usepackage{url}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{float}

\lstdefinestyle{snippet}{
    numbers = left,
    basicstyle = \ttfamily\footnotesize,
    frame = single,
    breaklines = true,
}

\setlength\parindent{0pt}

\begin{document}
\title{CS547 Advanced Topics in Computer Science\\
\large{Assignment 04 - Genetic Programming: Software Cost Estimation}}
\author{Aidan O'Grady - 201218150}
\date{}
\maketitle
\section{Introduction} % (fold)
\label{sec:introduction}
The purpose of this assignment was to utilise genetic programming in approaching
the problem of software cost estimation. When a software project is being
proposed, there are multiple factors that can influence the eventual cost of the
software. It can be difficult to determine which factors are more important to
consider, making this an appropriate search challenge in the scope of genetic
programming.

Using datasets provided, the primary goal was to identify which factors should
be taken into account to achieve the closest possible prediction to the cost
outlined in the file. Using various metrics, these estimations will be compared
to other methods that can be used to estimate costs such as linear regression.
% section introduction (end)

\section{Background} % (fold)
\label{sec:background}
There were around 15 .arff files available for use as part of this assignment.
Of those, I settled on using the \emph{Albrecht, Kemerer} \& \emph{Miyazaki}
datasets as part of my work. These three were chosen due to their relative
straightforwardness compared to other files. These datasets were also used by
Dolado \cite{Dolado200161} in a related study, which included results obtained
from his own genetic programming experimentation (providing me with references
to help understand the datasets). Knowing that I could use his results as a
reference point helped in ensuring that I stayed on the right track with this
problem. For determining which attributes to focus on for the datasets, I
decided to place my focus on numeric attributes, since I felt that including
the string or date attributes would have been difficult to include in the
calculations created by the genetic program.

\subsection{Albrecht} % (fold)
\label{sub:albrecht}
With the Albrecht dataset, I was able to understand the `function point' concept
used in various datasets. The original paper by Albrecht \& Gaffney
\cite{1703110}, showed how the \emph{RawFPcounts} attribute was calculated
from the \emph{Input, Output, Inquiry \& File} attributes, with a multiplier
then provided as well to add a modified value as well. With this dataset, I
decided to ignore the adjusted figure, instead opting to include the raw
function point value and its multiplier. I also opted to include the attributes
that make up the function points value, in case that the function points alone
could not provide the most accurate prediction.

Included: \emph{Input, Output, Inquiry, File, FPAdj, RawFPcounts}

Excluded: \emph{AdjFP}

Effort: \emph{Effort}
% subsection albrecht (end)

\subsection{Kemerer} % (fold)
\label{sub:kemerer}
The Kemerer dataset also included both raw and adjusted function point counts,
so the adjusted value was again ignored for the sake of consistency. The
\emph{ID} was ignored due to only being an identifier.

This dataset also included two attributes, \emph{Language} \& \emph{Hardware},
that acted as enumerations. This had to be addressed, since while these are going
to be factors in the real world, I was not certain whether I could trust the
enumerated values to be reliable in providing cost estimations. Thus, these were
also excluded. \emph{Duration} \emph{KSLOC} were difficult to decide on, due to
uncertainty as to their appropriateness. Eventually, I decided to exclude them,
due to this uncertainty.

Included: \emph{RAWFP}

Excluded: \emph{ID, Language, Hardware, Duration, KSLOC, AdjFP}

Effort: \emph{EffortMM}
% subsection kemerer (end)

\subsection{Miyazaki} % (fold)
\label{sub:miyazaki}
As with \emph{Kemerer}, the \emph{ID} and \emph{KLOC} attributes were excluded
for the same reasons. This meant that the only real decision to be made with
this set is which group of three attributes to choose. From reading the original
source \cite[Section~4.2]{MIYAZAKI19943}, I decided it was best to try with both
and choose whichever performed better with genetic programming. This became
\emph{SCRN, FORM} and \emph{FILE}. It may have been worth investigating some
combinations between them as well however.

Included: \emph{SCRN, FORM, FILE}

Excluded: \emph{ID, KLOC, ESCRN, EFORM, EFILE}

Effort: \emph{MM}
% subsection miyazaki (end)
% section background (end)

\section{Implementation Details} % (fold)
\label{sec:implementation_details}
The implementation of solving this problem came under three main section;
parsing the datasets, storing the values of these datasets into classes, and the
genetic programming parts as well. The genetic programming was handled with the
JGAP framework, meaning I was only required to implement the fitness function
and problem scope for it as well.

\subsection{Storing Data} % (fold)
\label{sub:storing_data}
The .arff files being used consist of three main aspects: the name of the
relation, the attributes and the data itself. As such, it was best to create two
classes for storing this data, \emph{DataRecord} and \emph{DataSet}.

\subsubsection{DataRecord} % (fold)
\label{ssub:datarecord}
This class was responsible for storing the data from a single record of the data
in the file. The class stored the actual effort of the record in addition to the
additional attributes. I felt that a \emph{Map} was the best way of storing a
record, using the name of the attribute as the key. I believed this allowed for
easier access to the values from just using the key, which I could use as
variable names in JGAP. As a result, the class also contains a method for
putting key/value pairs into the map.
% subsubsection datarecord (end)

\subsubsection{DataSet} % (fold)
\label{ssub:dataset}
The \emph{DataSet} class contained all the data within the file. As well as
including the name of the dataset for the sake of completion, two \emph{List}s
were stored for the attributes and data. As a result, methods for this class
were used for adding to getting from this list.
% subsubsection dataset (end)
% subsection storing_data (end)

\subsection{Parsing Data} % (fold)
\label{sub:parsing_data}
The \emph{DataParser} class requires three pieces of information in order to
parse the file: the file location, the list of attributes the user wishes to
include in the genetic program, and the attribute that the program is trying to
calculate. The list of attributes required for \emph{Data} is easily obtained,
since each relevant line starts with `@attribute', so the only concern was
ensuring that the attribute was numeric, achieved for string splitting.

Since the user is only concerned with a subset of the attributes, it was
important to find their indices in the attribute list, so that when the `@data'
section of the file was parsed, the correct values were being added to the
\emph{DataRecord} objects. Should the parser fail to parse, the parser simply
returns null, to be handled elsewhere.
% subsection parsing_data (end)

\subsection{JGAP} % (fold)
\label{sub:jgap}
The JGap implementation was based on a tutorial \cite{JGapTutorial} we were
directed to, with modifications to fit this problem scope. The JGap section was
made up of two parts, the \emph{CostProblem} and \emph{CostFitnessFunction}.

\subsubsection{CostProblem} % (fold)
\label{ssub:costproblem}
This class outlined the basic problem, containing the configuration of elements
such as the population size and program depths. The \emph{DataSet}'s attributes
were converted into a list of \emph{Variables} and the function set used in the
generated programs was expanded to addition, subtraction, multiplication,
division, power, exponential and logarithmic functions. Finally, the terminal
range was changed to floating point numbers between 0 and 100 for use as
functions.
% subsubsection costproblem (end)

\subsubsection{CostFitnessFunction} % (fold)
\label{ssub:costfitnessfunction}
The fitness function class is extremely simple. The fitness of a program
generated by JGAP is the \emph{Mean Magnitude of Relative Error} metric, defined
as follows:

\[\frac{\sum_{i = 1}^{n} \frac{abs(actual_i - estimated_i)}{actual_i}}{n}\]

Where \(n\) is the number of records in the dataset, \(actual_i\) is the effort
contained in the file for record \(i\) and \(estimated_i\) is the effort
calculated by the program. The fitness function ensures that the variables
are correctly set before each execution as well.
% subsubsection costfitnessfunction (end)
% subsection jgap (end)

\subsection{Application} % (fold)
\label{sub:application}
The \emph{Application} class commenced the evolution process. After parsing and
the \emph{CostProblem} is defined, JGAP performs evolution for 100 generations,
with time taken being recorded. When the evolution has finished, the best
performing program created is taken, and is used on the dataset once again to
obtain the predicted costs for each \emph{DataRecord}. This allows for the
calculation of \(PRED(n)\) scores where \(n = \{10, 25, 50\}\), allowing for a
final output outlining how many of the estimations fall within 10\%, 25\% or
50\% of the actual cost.

100 generations was chosen since it was found that anything beyond that saw
marginal improvements. For example, if the number of generations was set to 500,
then the number of improvements from generations 101 to 500 was too small to
justify the much larger length of time the evolution took.
% subsection application (end)
% section implementation_details (end)

\section{Results} % (fold)
\label{sec:results}
% section results (end)

\section{Comparisons} % (fold)
\label{sec:comparisons}
% section comparisons (end)
\newpage
\bibliographystyle{plain}
\bibliography{report} 
\end{document}