\documentclass[11pt, a4paper]{article}
\usepackage[parfill]{parskip}
\usepackage[margin=0.75in]{geometry}
\usepackage{url}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{float}
\usepackage{tabularx}
\usepackage{subfiles}

\lstdefinestyle{snippet}{
    numbers = left,
    basicstyle = \ttfamily\footnotesize,
    frame = single,
    breaklines = true,
}

\setlength\parindent{0pt}

\begin{document}
\title{CS547 Advanced Topics in Computer Science\\
\large{Assignment 04 - Genetic Programming: Software Cost Estimation}}
\author{Aidan O'Grady - 201218150}
\date{}
\maketitle
\section{Introduction} % (fold)
\label{sec:introduction}
The purpose of this assignment was to utilise genetic programming in approaching
the problem of software cost estimation. When a software project is being
proposed, there are multiple factors that can influence the eventual cost of the
software. It can be difficult to determine which factors are more important to
consider, making this an appropriate search challenge in the scope of genetic
programming. The goal of this assignment was to use genetic programming to
evolve programs to estimate costs of given datasets based on the attributes
with, comparing the resulting program's predictions with methods such as linear
regression.
% section introduction (end)

\section{Background} % (fold)
\label{sec:background}
There were around 15 .arff files available for use as part of this assignment.
Of those, I settled on using the \emph{Albrecht, Kemerer} \& \emph{Miyazaki}
datasets as part of my work. These three were chosen due to their relative
straightforwardness compared to other files. These datasets were also used by
Dolado \cite{Dolado200161} in a related study, which included results obtained
from his own genetic programming experimentation (providing me with references
to help understand the datasets). Knowing that I could use his results as a
reference point helped in ensuring that I stayed on the right track with this
problem. For determining which attributes to focus on for the datasets, I
decided to place my focus on numeric attributes, since I felt that including
the string or date attributes would have been difficult to include in the
calculations created by the genetic program.

\subsection{Albrecht} % (fold)
\label{sub:albrecht}
With the Albrecht dataset, I was able to understand the `function point' concept
used in various datasets. The original paper by Albrecht \& Gaffney
\cite{1703110}, showed how the \emph{RawFPcounts} attribute was calculated
from the \emph{Input, Output, Inquiry \& File} attributes, with a multiplier
then provided as well to add a modified value as well. With this dataset, I
decided to ignore the adjusted figure, instead opting to include the raw
function point value and its multiplier. I also opted to include the attributes
that make up the function points value, in case that the function points alone
could not provide the most accurate Prediction.

Included: \emph{Input, Output, Inquiry, File, FPAdj, RawFPcounts}

Excluded: \emph{AdjFP}

Effort: \emph{Effort}
% subsection albrecht (end)

\subsection{Kemerer} % (fold)
\label{sub:kemerer}
The Kemerer dataset also included both raw and adjusted function point counts,
so the adjusted value was again ignored for the sake of consistency. The
\emph{ID} was ignored due to only being an identifier.

This dataset also included two attributes, \emph{Language} \& \emph{Hardware},
that acted as enumerations. This had to be addressed, since while these are going
to be factors in the real world, I was not certain whether I could trust the
enumerated values to be reliable in providing cost estimations. Thus, these were
also excluded. \emph{Duration} \emph{KSLOC} were difficult to decide on, due to
uncertainty as to their appropriateness. Eventually, I decided to exclude them,
due to this uncertainty.

Included: \emph{RAWFP}

Excluded: \emph{ID, Language, Hardware, Duration, KSLOC, AdjFP}

Effort: \emph{EffortMM}
% subsection kemerer (end)

\subsection{Miyazaki} % (fold)
\label{sub:miyazaki}
As with \emph{Kemerer}, the \emph{ID} and \emph{KLOC} attributes were excluded
for the same reasons. This meant that the only real decision to be made with
this set is which group of three attributes to choose. From reading the original
source \cite[Section~4.2]{MIYAZAKI19943}, I decided it was best to try with both
and choose whichever performed better with genetic programming. This became
\emph{SCRN, FORM} and \emph{FILE}. It may have been worth investigating some
combinations between them as well however.

Included: \emph{SCRN, FORM, FILE}

Excluded: \emph{ID, KLOC, ESCRN, EFORM, EFILE}

Effort: \emph{MM}
% subsection miyazaki (end)
% section background (end)

\section{Implementation Details} % (fold)
\label{sec:implementation_details}
The implementation of solving this problem came under three main section;
parsing the datasets, storing the values of these datasets into classes, and the
genetic programming parts as well. The genetic programming was handled with the
JGAP framework, meaning I was only required to implement the fitness function
and problem scope for it as well. The linear regression was performed using
Weka.

\subsection{Storing Data} % (fold)
\label{sub:storing_data}
The .arff files being used consist of three main aspects: the name of the
relation, the attributes and the data itself. As such, it was best to create two
classes for storing this data, \emph{DataRecord} and \emph{DataSet}.

\subsubsection{DataRecord} % (fold)
\label{ssub:datarecord}
This class was responsible for storing the data from a single record of the data
in the file. The class stored the actual effort of the record in addition to the
additional attributes. I felt that a \emph{Map} was the best way of storing a
record, using the name of the attribute as the key. I believed this allowed for
easier access to the values from just using the key, which I could use as
variable names in JGAP. As a result, the class also contains a method for
putting key/value pairs into the map.
% subsubsection datarecord (end)

\subsubsection{DataSet} % (fold)
\label{ssub:dataset}
The \emph{DataSet} class contained all the data within the file. As well as
including the name of the dataset for the sake of completion, two \emph{List}s
were stored for the attributes and data. As a result, methods for this class
were used for adding to getting from this list.
% subsubsection dataset (end)
% subsection storing_data (end)

\subsection{Parsing Data} % (fold)
\label{sub:parsing_data}
The \emph{DataParser} class requires three pieces of information in order to
parse the file: the file location, the list of attributes the user wishes to
include in the genetic program, and the attribute that the program is trying to
calculate. The list of attributes required for \emph{Data} is easily obtained,
since each relevant line starts with `@attribute', so the only concern was
ensuring that the attribute was numeric, achieved for string splitting.

Since the user is only concerned with a subset of the attributes, it was
important to find their indices in the attribute list, so that when the `@data'
section of the file was parsed, the correct values were being added to the
\emph{DataRecord} objects. Should the parser fail to parse, the parser simply
returns null, to be handled elsewhere.
% subsection parsing_data (end)

\subsection{JGAP} % (fold)
\label{sub:jgap}
The JGap implementation was based on a tutorial \cite{JGapTutorial} we were
directed to, with modifications to fit this problem scope. The JGap section was
made up of two parts, the \emph{CostProblem} and \emph{CostFitnessFunction}.

\subsubsection{CostProblem} % (fold)
\label{ssub:costproblem}
This class outlined the basic problem, containing the configuration of elements
such as the population size and program depths. The \emph{DataSet}'s attributes
were converted into a list of \emph{Variables} and the function set used in the
generated programs was expanded to addition, subtraction, multiplication,
division, power, exponential and logarithmic functions. Finally, the terminal
range was changed to floating point numbers between 0 and 100 for use as
functions.
% subsubsection costproblem (end)

\subsubsection{CostFitnessFunction} % (fold)
\label{ssub:costfitnessfunction}
The fitness function class is extremely simple. The fitness of a program
generated by JGAP is the \emph{Mean Magnitude of Relative Error} (MMRE) metric,
defined as follows:

\[\frac{\sum_{i = 1}^{n} \frac{abs(actual_i - estimated_i)}{actual_i}}{n}\]

Where \(n\) is the number of records in the dataset, \(actual_i\) is the effort
contained in the file for record \(i\) and \(estimated_i\) is the effort
calculated by the program. The fitness function ensures that the variables
are correctly set before each execution as well. The lower the fitness value,
the better it is.
% subsubsection costfitnessfunction (end)
% subsection jgap (end)

\subsection{Application} % (fold)
\label{sub:application}
The \emph{Application} class commenced the evolution process. After parsing and
the \emph{CostProblem} is defined, JGAP performs evolution for 100 generations,
with time taken being recorded. When the evolution has finished, the best
performing program created is taken, and is used on the dataset once again to
obtain the Predicted costs for each \emph{DataRecord}. This allows for the
calculation of \(Pred(n)\) scores where \(n = \{10, 25, 50\}\), allowing for a
final output outlining how many of the estimations fall within 10\%, 25\% or
50\% of the actual cost.

100 generations was chosen since it was found that anything beyond that saw
marginal improvements. For example, if the number of generations was set to 500,
then the number of improvements from generations 101 to 500 was too small to
justify the much larger length of time the evolution took.
% subsection application (end)
% section implementation_details (end)
\newpage
\section{Results} % (fold)
\label{sec:results}
These results are summation of the overall performance of JGap and Weka on the
three datasets. Detailed results can be found in Appendix
\ref{sec:detailed_results}, showing each data record's performance.
Constants have been rounded to two decimal places, with minimal simplification
of the returned programs applied.

The JGap configuration saw a population size of 5000, a maximum initial depth of
10, and a maximum crossover depth of 20. The population was evolved for 100
generations.

\subsection{Albrecht Results} % (fold)
\label{sub:albrecht_results}
Number of records: 24

GP:
\(((RawFPcounts / 66.2) + ((log (log (Output * (log File)))) +
(FPAdj ^ {Inquiry}))) * (log (log File))\)

LR:
\(0.16 * Input + 0.33 * Output + 0.50 * Inquiry + 0.35 * File - 14.87\)

\begin{tabularx}{\textwidth}{| l | X | X |}
\hline
 & \textbf{Genetic Program} & \textbf{Linear Regression} \\
\hline
\emph{MMRE} & 0.351 & 0.883 \\
\hline
\emph{Pred(10)} & 08/24 (33.3\%) & 05/24 (20.8\%) \\
\emph{Pred(25)} & 13/24 (54.2\%) & 09/24 (37.5\%) \\
\emph{Pred(50)} & 18/24 (75.0\%) & 12/24 (50.0\%) \\
\hline
\emph{Time (s)} & 19.426 & 0.01 \\
\hline
\end{tabularx}
% subsection albrecht_results (end)

\subsection{Kemerer Results} % (fold)
\label{sub:kemerer_results}
Number of records: 15

GP: \((RAWFP \div Duration) + (5 \times Duration \div ((RAWFP \div Duration)
\div Duration))\)

LR: \(0.33 \times RAWFP - 111.37\)

\begin{tabularx}{\textwidth}{| l | X | X |}
\hline
 & \textbf{Genetic Program} & \textbf{Linear Regression} \\
\hline
\emph{MMRE} & 0.354 & 0.874 \\
\hline
\emph{Pred(10)} & 04/15 (26.7\%) & 00/15 (00.0\%) \\
\emph{Pred(25)} & 06/15 (40.0\%) & 03/15 (20.0\%) \\
\emph{Pred(50)} & 10/15 (66.7\%) & 06/15 (40.0\%) \\
\hline
\emph{Time (s)} & 19.426 & 0.01 \\
\hline
\end{tabularx}
% subsection kemerer_results (end)

\subsection{Miyazaki Results} % (fold)
\label{sub:miyazaki_results}
Number of records: 48

GP: \((Exp(0.38)) + ((((Exp((Exp(0.5)))) + FORM) / (1.27 + SCRN)) + SCRN)\)

LR: \(0.4445 * SCRN + 0.84 * FORM + 3.59 * FILE - 71.13\)

\begin{tabularx}{\textwidth}{| l | X | X |}
\hline
 & \textbf{Genetic Program} & \textbf{Linear Regression} \\
\hline
\emph{MMRE} & 0.410 & 1.942 \\
\hline
\emph{Pred(10)} & 11/48 (22.9\%) & 02/48 (04.2\%) \\
\emph{Pred(25)} & 19/48 (39.6\%) & 08/48 (16.7\%) \\
\emph{Pred(50)} & 27/48 (56.3\%) & 13/48 (27.1\%) \\
\hline
\emph{Time (s)} & 22.039 & 0.01 \\
\hline
\end{tabularx}
% subsection miyazaki_results (end)
% section results (end)

\section{Comparisons} % (fold)
\label{sec:comparisons}
\subsection{Albrecht} % (fold)
\label{sub:albrecht_comp}
The \emph{Albrecht} dataset saw strong results with JGap, resulting in an MMRE
score of 0.351. The Pred scores were strong across the board, with a third of
the dataset having estimates within 10\% of the actual effort, and a majority
falling within 25\%. This is a far stronger showing compared to Weka's linear
regression, which saw an MMRE of 0.883 instead. Weka's Pred(10) and Pred(25)
scores did not match up to those obtained by JGap, although the difference is
not as wide as compared to that between the MMRE scores, suggesting that there
is little correlation between them.

In terms of the complexity of the programs produced by these methods, it is
interesting to note that Weka relied on basic arithmetic functions, while
JGap took full advantage of the full range of functions available. It is
interesting to note that JGap felt that the \emph{Input} attribute was not
required to obtain the best result, while JGap did not use the RawFPcount or
FPAdj despite being available.
% subsection albrecht_comp (end)

\subsection{Kemerer} % (fold)
\label{sub:kemerer_comp}
The results obtained from the \emph{Kemerer} were similar to those obtained from
\emph{Albrecht} when it comes to MMRE, despite having very different attributes
available. However, the Pred scores and produced programs are different from the
previous dataset.

The Pred scores again show much better performance for JGap. While a quarter of
its estimates fall within 10\%, Weka failed to have any estimation fall within
this range. Similarly, JGap estimated twice as many wthin 25\% as Weka did. This
is a strong highlight of genetic programming's performance in this scenario.

Perhaps due to only having a single attribute, the programs produced by both
methods are much simpler. JGap did not rely on any constant values at all in
the produced program, nor did it go beyond the basic arithmetic functions in
this instance. Linear regression's program is still simpler, requiring only two
operations compared to the six required for the genetic programming solution.
However, it is clear that this complexity is necessary to improve performance
once again.
% subsection kemerer_comp (end)

\subsection{Miyazaki} % (fold)
\label{sub:miyazaki_comp}
The results that JGap produced for the \emph{Miyazaki} dataset were not as
strong as those for the previous datasets, although they are still effective
results to obtain. While it produced an MMRE score of 0.41, Weka completely
failed to handle the dataset, with an MMRE score of 1.942, suggest some wildly
inaccurate predictions were found.

In terms of the Pred scores, JGap convincingly outperformed Weka. Its Pred(10)
score of 23\% was much better than Weka's 4.2\%, while it performed twice as
well for Pred(25), with 39.6\% compared to 16.7\%.

When looking at the program itself, the first notable feature is JGap's ignoring
of the \emph{FILE} attribute. What is also interesting that JGap's solution only
used a single extra operation compared to Weka, although it should be noted that
JGap did include more complex operations.
% subsection miyazaki_comp (end)

\subsection{Conclusions} % (fold)
\label{sub:conclusions}
The overall conclusion is that JGap's genetic programming overwhelmingly
outperformed Weka's linear regression. Genetic programming can typically
estimate somewhere 20\% and 33\% of all records within 10\%, and between 40\%
and 60\% within 50\%. JGap required more processing time to come up with its
best results, taking advantage of more complex functions in order to achieve
its better results. While JGap was prone to the very rare abnormally inaccurate
estimation (such as \emph{Albrecht} 21 and \emph{Miyazaki} 16, each having
magnitudes of relative errors of over 2, meaning that the estimations were
off by more than two times the actual effort), this was a very frequent issue
for Weka to deal with with its produced results. 
% subsection conclusions (end)
% section comparisons (end)

\newpage
\appendix
\section{Detailed Results} % (fold)
\label{sec:detailed_results}
Note that the ID field refers to the order of records in the file, not the ID
attribute within the file.

\subfile{res/albrecht}
\subfile{res/kemerer}
\subfile{res/miyazaki}
% section detailed_results (end)
\bibliographystyle{plain}
\bibliography{report} 
\end{document}